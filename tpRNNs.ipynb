{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 4 - Réseaux de neurones récurrents\n",
    "\n",
    "**L’objectif de ce TP est d’utiliser des réseaux de neurones récurrents pour l’analyse de données séquentielles.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : Génération de poésie\n",
    "\n",
    "Une première application va consister à apprendre à générer du texte. Nous allons partir d’une base de données d’un recueil de poésies, « les fleurs de mal » de Charles Baudelaire.\n",
    "On pourra récupérer le fichier d’entrée à l’adresse suivante: [http://cedric.cnam.fr/~thomen/cours/US330X/fleurs_mal.txt](http://cedric.cnam.fr/~thomen/cours/US330X/fleurs_mal.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get  # to make GET request\n",
    "\n",
    "def download(url, file_name):\n",
    "    # open in binary mode\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url)\n",
    "        # write to file\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('http://cedric.cnam.fr/~thomen/cours/US330X/fleurs_mal.txt',\"fleurs_mal.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a) Génération des données et étiquettes**\n",
    "\n",
    "On créera un script `exo0.py` pour générer les données et étiquettes. On va commencer par parser le ficher d’entrée pour récupérer le texte et effectuer quelques pré-traitements simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "bStart = False\n",
    "fin = open(\"fleurs_mal.txt\", 'r' , encoding = 'utf8')\n",
    "lines = fin.readlines()\n",
    "lines2 = []\n",
    "text = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip().lower() # Remove blanks and capitals\n",
    "    if(\"Charles Baudelaire avait un ami\".lower() in line and bStart==False):\n",
    "        print(\"START\")\n",
    "        bStart = True\n",
    "    if(\"End of the Project Gutenberg EBook of Les Fleurs du Mal, by Charles Baudelaire\".lower() in line):\n",
    "        print(\"END\")\n",
    "        break\n",
    "        \n",
    "    if(bStart==False or len(line) == 0):\n",
    "        continue\n",
    "        \n",
    "    lines2.append(line)\n",
    "\n",
    "fin.close()\n",
    "text = \" \".join(lines2)\n",
    "chars = sorted(set([c for c in text]))\n",
    "nb_chars = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "\n",
    "Comment s’interprète la variable `chars` ? Que représente `nb_chars` ?\n",
    "\n",
    "Dans la suite, on va considérer chaque caractère du texte d’entrée par un encodage *one-hot* sur le dictionnaire de symboles. **On va appliquer un réseau de neurones récurrent qui va traiter une séquence de SEQLEN caractères, et dont l’objectif va être de prédire le caractère suivant en fonction de la séquence courante.** On se situe donc dans le cas d’un problème d’apprentissage *auto-supervisé*, *i.e.* qui ne contient pas de label mais dont on va construire artificiellement une supervision.\n",
    "\n",
    "Les données d’entraînement consisteront donc en un ensemble de séquences d’entraînement de taille SEQLEN, avec une étiquette cible correspondant au prochain caractère à prédire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "SEQLEN = 10 # Length of the sequence to predict next char\n",
    "STEP = 1 # stride between two subsequent sequences\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    # Append input of size SEQLEN\n",
    "    # Append output (label) of size 1\n",
    "nbex = len(input_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant vectoriser les données d’entraînement en utilisant le dictionnaire et un encodage *one-hot* pour chaque caractère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# mapping char -> index in dictionary: used for encoding (here)\n",
    "char2index = dict((c, i) for i, c in enumerate(chars))\n",
    "# mapping char -> index in dictionary: used for decoding, i.e. generation - part c)\n",
    "index2char = dict((i, c) for i, c in enumerate(chars)) # mapping index -> char in dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque séquence d’entraînement est donc représentée par une matrice de taille $ SEQLEN \\times tdict $, correspondant à une longueur de $ SEQLEN $ caractères, chaque caratère étant encodé par un vecteur binaire correspondant à un encodage *one-hot*.\n",
    "\n",
    "- L’ensemble des données d’entraînement `X` seront donc constituées par un tenseur de taille $ nbex \\times SEQLEN \\times tdict $  \n",
    "- L’ensemble des labels d’entraînement `y` seront représentées par un tenseur de $ nbex \\times tdict $, où la sortie pour chaque exemple correspond à l’indice dans le dictionnaire du caractère suivant la séquence  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "\n",
    "Compléter le code suivant pour créer les données et labels d’entraînement. **N.B.** : utiliser la variable `char2index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        # Fill X at correct index\n",
    "     # Fill y at correct index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant séparer les données en deux ensembles d’apprentissage et de test, et les sauvegarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "\n",
    "ratio_train = 0.8\n",
    "nb_train = int(round(len(input_chars)*ratio_train))\n",
    "print(\"nb tot=\",len(input_chars) , \"nb_train=\",nb_train)\n",
    "X_train = X[0:nb_train,:,:]\n",
    "y_train = y[0:nb_train,:]\n",
    "\n",
    "X_test = X[nb_train:,:,:]\n",
    "y_test = y[nb_train:,:]\n",
    "print(\"X train.shape=\",X_train.shape)\n",
    "print(\"y train.shape=\",y_train.shape)\n",
    "\n",
    "print(\"X test.shape=\",X_test.shape)\n",
    "print(\"y test.shape=\",y_test.shape)\n",
    "\n",
    "outfile = \"Baudelaire_len_\"+str(SEQLEN)+\".p\"\n",
    "\n",
    "with open(outfile, \"wb\" ) as pickle_f:\n",
    "    pickle.dump( [index2char, X_train, y_train, X_test, y_test], pickle_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b) Apprentissage d’un modèle auto-supervisé pour la génération de texte**\n",
    "\n",
    "On va maintenant créer `exo1.py` pour entraîner un réseau de neurone récurrent. On va commencer par charger les données précédentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "SEQLEN = 10\n",
    "outfile = \"Baudelaire_len_\"+str(SEQLEN)+\".p\"\n",
    "[index2char, X_train, y_train, X_test, y_test] = pickle.load( open( outfile, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis créer un modèle séquentiel :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on va ajouter une couche récurrente avec un modèle de type `SimpleRNN` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "HSIZE = 128\n",
    "model.add(SimpleRNN(HSIZE, return_sequences=False, input_shape=(SEQLEN, nb_chars),unroll=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "\n",
    "- Expliquer à quoi correspond `return_sequences=False`. **N.B.** : `unroll=True` permettra simplement d’accélérer les calculs.  \n",
    "\n",
    "\n",
    "On ajoutera enfin une couche complètement connectée suivie d’une fonction `softmax` for effectuer la classification du caractère suivant la séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# ADD FULLY CONNECTED LAYER (output size ?)\n",
    "# ADD SOFTMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour optimiser des réseaux récurrents, on utilise préférentiellement des méthodes adaptatives comme `RMSprop` [[TH12]](#tieleman2012). On pourra donc compiler le modèle et utiliser la méthode `summary()` pour visualiser le nombre de paramètres du réseaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "learning_rate = 0.001\n",
    "# CREATE OPTIMIZER & COMPILE\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’entraînement sera effectuer comme habituellement avec la méthode `fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# FIT MODEL TO DATA\n",
    "\n",
    "# EVALUATE TRAINED MODEL\n",
    "scores_train = model.evaluate(X_train, y_train, verbose=1)\n",
    "scores_test = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"PERFS TRAIN: %s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "print(\"PERFS TEST: %s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra utiliser la méthode `saveModel` pour stocker le modèle appris :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "def saveModel(model, savename):\n",
    "    # serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(savename+\".yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "        print(\"Yaml Model \",savename,\".yaml saved to disk\")\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(savename+\".h5\")\n",
    "    print(\"Weights \",savename,\".h5 saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de l’apprentissage\n",
    "\n",
    "Quelles taux de classification obtient-on en apprentissage ? Commenter les performances obtenues. En quoi le problème est-il différents des problèmes de classification abordés jusqu’ici ? Par exemple, faire une recherche de la séquence d’entrée « la mort de », et analyser les labels cibles présents dans le corpus d’apprentissage.\n",
    "\n",
    "\n",
    "<a id='text-gen'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c) Génération de texte avec le modèle appris**\n",
    "\n",
    "> On va maintenant se servir du modèle précédemment entraîné pour générer du texte qui va « imiter » le style du corpus de poésie sur lequel il a été appris.\n",
    "On mettre en place un script `exo2.py` pour cette partie.\n",
    "\n",
    "On va commencer par charger les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "SEQLEN = 10\n",
    "outfile = \"Baudelaire_len_\"+str(SEQLEN)+\".p\"\n",
    "[index2char, X_train, y_train, X_test, y_test] = pickle.load( open( outfile, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et le réseau récurrent avec la fonction `loadModel` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "def loadModel(savename):\n",
    "    with open(savename+\".yaml\", \"r\") as yaml_file:\n",
    "        model = model_from_yaml(yaml_file.read())\n",
    "    print(\"Yaml Model \",savename,\".yaml loaded \")\n",
    "    model.load_weights(savename+\".h5\")\n",
    "    print(\"Weights \",savename,\".h5 loaded \")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra vérifier l’architecture du réseau avec la méthode `summary`, et évaluer les performances :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model = loadModel(nameModel)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='RMSprop',metrics=['accuracy'])\n",
    "model.summary()\n",
    "nb_chars = len(index2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant sélectionner une chaîne de caractère initiale pour notre réseau, afin de prédire le caractère suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "seed =15608\n",
    "char_init = \"\"\n",
    "for i in range(SEQLEN):\n",
    "    char = index2char[np.argmax(X_train[seed,i,:])]\n",
    "    char_init += char\n",
    "\n",
    "print(\"CHAR INIT: \"+char_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va convertir la séquence de départ au format *one-hot* pour appliquer le modèle de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "test = np.zeros((1, SEQLEN, nb_chars), dtype=np.bool)\n",
    "test[0,:,:] = X_train[seed,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au lieu de prédire directement la sortie de probabilité maximale, on va échantillonner une sortie tirée selon la distribution de probabilités du soft-max.\n",
    "Pour commencer on va utiliser un paramètre de température pour rendre la distribution plus ou moins piquée. On va transformer la distribution en sortie du soft-max de la façon suivante :\n",
    "\n",
    "\n",
    "<a id='equation-normalisation-temperature'></a>\n",
    "$$\n",
    "z_{i}^N  = \\frac{z_{i}^{\\frac{1}{T}}}{\\sum\\limits_{j=1}^C z_{j}^{\\frac{1}{T}} } \\tag{1}\n",
    "$$\n",
    "\n",
    "On pourra utiliser la fonction suivante pour effectuer l’échantillonage après transformation de distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def sampling(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    predsN = pow(preds,1.0/temperature)\n",
    "    predsN /= np.sum(predsN)\n",
    "    probas = np.random.multinomial(1, predsN, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figure ci-dessous montre l’impact sur la distribution de cette renormalisation :\n",
    "\n",
    "<img src=\"http://cedric.cnam.fr/~thomen/cours/US330X/_images/temperature.png\" style=\"height:280px;\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Quel va être le comportement de cet échantillonnage lorsque la température T augmente ($ T \\rightarrow +\\infty $) ou diminue ($ T \\rightarrow 0 $) ?  \n",
    "\n",
    "\n",
    "**On va maintenant mettre en place la génération de texte à partir d’une séquence de SEQLEN caractère initiaux.** Compléter le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "nbgen = 400 # number of characters to generate (1,nb_chars)\n",
    "gen_char = char_init\n",
    "temperature  = 0.5\n",
    "\n",
    "for i in range(nbgen):\n",
    "    preds = model.predict(test)[0]  # shape (1,nb_chars)\n",
    "    next_ind = # SAMPLING\n",
    "    next_char = # CONCERT INDEX -> CHAR\n",
    "    gen_char += next_char\n",
    "    for i in range(SEQLEN-1):\n",
    "        test[0,i,:] = test[0,i+1,:]\n",
    "    test[0,SEQLEN-1,:] = 0\n",
    "    test[0,SEQLEN-1,next_ind] = 1\n",
    "\n",
    "print(\"Generated text: \"+gen_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de la génération\n",
    "\n",
    "Evaluer l’impact du paramètre de température dans la génération, ainsi que le nombre d’époques dans l’apprentissage. Commenter les points forts et points faibles du générateur.\n",
    "\n",
    "\n",
    "<a id='tp4-embedding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : Embedding Vectoriel de texte\n",
    "\n",
    "Dans cet exercice, nous allons explorer l’embedding vectoriel de texte Glove [[PSM14]](#pennington14glove-global) qui sera utilisé dans la TP suivant pour décrire chaque mot d’un corpus dans un objectif de légendage d’images.\n",
    "\n",
    "On va utiliser la base d’image FlickR8k ([http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html](http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html)), pour laquelle chaque image est associée à 5 légendes différentes qui décrivent son contenu en langage naturel.\n",
    "\n",
    "On va commencer par télécharger le fichier qui contient les légendes de la base d’image Flickr 8k : [http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_train_dataset.txt](http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_train_dataset.txt). La base d’apprentissage contient 6000 images, ce qui correspond à 30000 légendes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a) Extraction des embedding Glove des légendes**\n",
    "\n",
    "On va créer un script `exo3.py` pour extraire les embedding vectoriaux Glove des légendes de la base Glove.\n",
    "On utilisera le code suivant pour récupérer l’ensemble des mots présents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_train_dataset.txt',\"flickr_8k_train_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = 'flickr_8k_train_dataset.txt'\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "nb_samples = df.shape[0]\n",
    "iter = df.iterrows()\n",
    "allwords = []\n",
    "for i in range(nb_samples):\n",
    "    x = iter.__next__()\n",
    "    cap_words = x[1][1].split() # split caption into words\n",
    "    cap_wordsl = [w.lower() for w in cap_words] # remove capital letters\n",
    "    allwords.extend(cap_wordsl)\n",
    "\n",
    "unique = list(set(allwords)) # List of different words in captions\n",
    "print(len(unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant télécharger le fichier contenant les Embeddings vectoriels Glove :  [http://cedric.cnam.fr/~thomen/cours/US330X/glove.6B.100d.txt](http://cedric.cnam.fr/~thomen/cours/US330X/glove.6B.100d.txt) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('http://cedric.cnam.fr/~thomen/cours/US330X/glove.6B.100d.txt',\"glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "GLOVE_MODEL = \"glove.6B.100d.txt\"\n",
    "fglove = open(GLOVE_MODEL, \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On déterminer la liste des mots présents dans les légendes et dans le fichier Glove. Compléter le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cpt=0\n",
    "for line in fglove:\n",
    "    row = line.strip().split()\n",
    "    # word = COMPLETE WITH YOUR CODE\n",
    "    if(word in unique or word=='unk'):\n",
    "        listwords.append(word)\n",
    "    # embedding = COMPLETE WITH YOUR CODE - use a numpy array with dtype=\"float32\"\n",
    "    listembeddings.append(embedding)\n",
    "\n",
    "    cpt +=1\n",
    "    print(\"word: \"+word+\" embedded \"+str(cpt))\n",
    "\n",
    "fglove.close()\n",
    "nbwords = len(listembeddings)\n",
    "tembedding = len(listembeddings[0])\n",
    "print(\"Number of words=\"+str(len(listembeddings))+\" Embedding size=\"+str(tembedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B. :** on a ajouté le mot “unk” qui est destiné à coder les mots des légendes absents du fichiers d’embedding.\n",
    "\n",
    "On va finalement créer la matrice des embedding, en ajoutant deux mots pour coder les mots “start” et “end” (utile pour le TP suivant) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(listembeddings)+2,tembedding+2))\n",
    "for i in range(nbwords):\n",
    "    embeddings[i,0:tembedding] = listembeddings[i]\n",
    "\n",
    "listwords.append('<start>')\n",
    "embeddings[7001,100] = 1\n",
    "# APPEND <end> symbol\n",
    "# FILL embeddings as requested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "\n",
    "Expliquer la taille et le contenu de la matrice embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et sauvegarder la liste des mots et les vecteurs associés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "\n",
    "outfile = 'Caption_Embeddings.p'\n",
    "with open(outfile, \"wb\" ) as pickle_f:\n",
    "    pickle.dump( [listwords, embeddings], pickle_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b) Analyse des embedding Glove des légendes**\n",
    "\n",
    "On va commencer par ouvrir le fichier des embeddings, puis à normaliser les vecteurs pour qu’ils aient une norme euclidienne unité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "\n",
    "outfile = 'Caption_Embeddings.p'\n",
    "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) )\n",
    "print(\"embeddings: \"+str(embeddings.shape))\n",
    "\n",
    "for i in range(embeddings.shape[0]):\n",
    "    # l2 NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "\n",
    "Expliquer l’objectif de la normalisation\n",
    "\n",
    "On va maintenant effectuer un clustering dans l’espace des embeddings en 10 groupes avec l’algorithme du KMeans : [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). On utilisera max_iter=1000 et init=”random”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = # COMPLETE WITH YOUR CODE - apply fit() method on embeddings\n",
    "clustersID  = kmeans.labels_\n",
    "clusters = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d’afficher les points le point le plus proche de chaque centre, ainsi que les 20 points suivants les plus proche du centre, on pourra utiliser le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "pointsclusters = # INIT - COMPLETE WITH YOUR CODE\n",
    "indclusters = # INIT - COMPLETE WITH YOUR CODE\n",
    "\n",
    "for i in range(10):\n",
    "    norm = np.linalg.norm((clusters[i] - embeddings),axis=1)\n",
    "    inorms = np.argsort(norm)\n",
    "    indclusters[i][:] = inorms[:]\n",
    "    \n",
    "    print(\"Cluster \"+str(i)+\" =\"+listwords[indclusters[i][0]])\n",
    "    for j in range(1,21):\n",
    "        print(\" mot: \"+listwords[indclusters[i][j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "\n",
    "**Montrer le résultat des centre du clustering obtenu ainsi que les plus proches de chaque centre. Commenter le résultat par rapport à la sémantique des mots.**\n",
    "\n",
    "Pour visualiser la répartition des points dans l’espace d’embedding, on pourra utiliser la méthode t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=30, verbose=2, init='pca', early_exaggeration=24)\n",
    "points2D = tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et afficher les points des différents clusters ainsi que le centre avec un croix ainsi :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    pointsclusters[i,:] = points2D[int(indclusters[i][0])]\n",
    "\n",
    "cmap =cm.tab10\n",
    "plt.figure(figsize=(3.841, 7.195), dpi=100)\n",
    "plt.set_cmap(cmap)\n",
    "plt.subplots_adjust(hspace=0.4 )\n",
    "plt.scatter(points2D[:,0], points2D[:,1], c=clustersID,  s=3,edgecolors='none', cmap=cmap, alpha=1.0)\n",
    "plt.scatter(pointsclusters[:,0], pointsclusters[:,1], c=range(10),marker = '+', s=1000, edgecolors='none', cmap=cmap, alpha=1.0)\n",
    "\n",
    "plt.colorbar(ticks=range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='pennington14glove-global'></a>\n",
    "\\[PSM14\\] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: global vectors for word representation. In *In EMNLP*. 2014.\n",
    "\n",
    "<a id='tieleman2012'></a>\n",
    "\\[TH12\\] T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012."
   ]
  }
 ],
 "metadata": {
  "date": 1637163670.7966561,
  "filename": "tpRNNs.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "title": "TP 4 - Réseaux de neurones récurrents"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
